{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.chdir('/home/jcsanguino10/local_citation_model/recommender-fusion-recsys/loaders')\n",
    "from create_dataloader_sequential import (load_course_encoder)\n",
    "\n",
    "os.chdir('/home/jcsanguino10/local_citation_model/recommender-fusion-recsys/architectures/Sequence')\n",
    "from sec_transformer_pytorch import (create_model, load_pytorch_weights)\n",
    "\n",
    "os.chdir('/home/jcsanguino10/local_citation_model/recommender-fusion-recsys/architectures/Multimodal')\n",
    "from multimodal import Autoencoder, MultimodalModel\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caddf8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Change to just a particular GPU changing the enviroment variable\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    # Change to just use a particular GPU via torch\n",
    "    #torch.cuda.set_device(\"cuda:3\")\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9edd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths from the label encoder dicts \n",
    "\n",
    "PATH_TO_LABEL_ENCODER = '/home/jcsanguino10/local_citation_model/data/processed/'\n",
    "\n",
    "# Path to folder with datasets\n",
    "\n",
    "PATH_TO_DATASETS = '/home/jcsanguino10/local_citation_model/data/'\n",
    "\n",
    "# Path to folder with checkpoints best models\n",
    "\n",
    "PATH_TO_CHECKPOINTS = '/home/jcsanguino10/local_citation_model/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44862572",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder, dicts = load_course_encoder('/home', PATH_TO_LABEL_ENCODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_encoder.classes_)  # List of all course IDs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177798f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_binary = pd.read_pickle(f'{PATH_TO_DATASETS}train_binary_all_vectors_128_01_transe_seqvec.pkl')\n",
    "df_bpr_df = pd.read_pickle(f'{PATH_TO_DATASETS}train_bpr_all_vectors_128_01_transe_seqvec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa3b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_columns_to_tensor(df, columns, new_column_name):\n",
    "    \"\"\"\n",
    "    Concatenates specified columns in a DataFrame and creates a tensor.\n",
    "    The resulting tensor is saved in a new column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to concatenate.\n",
    "        new_column_name (str): Name of the new column to store the tensor.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the new column containing tensors.\n",
    "    \"\"\"\n",
    "    df[new_column_name] = df[columns].apply(\n",
    "        lambda row: torch.tensor([item for col in columns for item in row[col]], dtype=torch.float),\n",
    "        axis=1\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b183f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = concat_columns_to_tensor(df_binary, ['item_text_embedding', 'item_bpr_embedding', 'item_graph_embedding'], 'course_full_embeddings')\n",
    "df_binary = concat_columns_to_tensor(df_binary, ['user_text_embedding', 'user_bpr_embedding', 'user_graph_embedding', 'user_sequence_embedding'], 'user_full_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bpr_df = concat_columns_to_tensor(df_bpr_df, ['pos_item_text_embedding', 'pos_item_bpr_embedding', 'pos_item_graph_embedding'], 'pos_course_full_embeddings')\n",
    "df_bpr_df = concat_columns_to_tensor(df_bpr_df, ['neg_item_text_embedding', 'neg_item_bpr_embedding', 'neg_item_graph_embedding'], 'neg_course_full_embeddings')\n",
    "df_bpr_df = concat_columns_to_tensor(df_bpr_df, ['user_text_embedding', 'user_bpr_embedding', 'user_graph_embedding', 'user_sequence_embedding'], 'user_full_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092eee90",
   "metadata": {},
   "source": [
    "# Autoencoder training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_and_extract_encoder(data, input_dim, encoding_dims, epochs=50, lr=1e-3, \n",
    "                                         save_path=None, device='cuda', verbose=True):\n",
    "    \"\"\"\n",
    "    Train an autoencoder.\n",
    "    \n",
    "    Args:\n",
    "        data (torch.Tensor): Training data tensor of shape (batch_size, input_dim)\n",
    "        input_dim (int): Dimension of input features\n",
    "        encoding_dims (list): List of hidden layer dimensions for encoder\n",
    "                             Example: [512, 256, 128] for 3-layer encoder\n",
    "        epochs (int): Number of training epochs\n",
    "        lr (float): Learning rate\n",
    "        save_path (str): Path to save the best autoencoder model (optional)\n",
    "        device (str): Device to train on ('cpu' or 'cuda')\n",
    "        verbose (bool): Whether to print training progress\n",
    "        \n",
    "    Returns:\n",
    "        encoder (nn.Module): The trained autoencoder model\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üöÄ Starting autoencoder training...\")\n",
    "        print(f\"Input dimension: {input_dim}\")\n",
    "        print(f\"Encoding dimensions: {encoding_dims}\")\n",
    "        print(f\"Final encoding dimension: {encoding_dims[-1]}\")\n",
    "        print(f\"Training data shape: {data.shape}\")\n",
    "    \n",
    "    # Create autoencoder\n",
    "    autoencoder = Autoencoder(input_dim=input_dim, encoding_dims=encoding_dims)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä Autoencoder architecture created\")\n",
    "        print(f\"Encoder layers: {len(autoencoder.encoder)}\")\n",
    "        print(f\"Decoder layers: {len(autoencoder.decoder)}\")\n",
    "    \n",
    "    # Train the autoencoder using the enhanced train_autoencoder method\n",
    "    trained_autoencoder = Autoencoder.train_autoencoder(\n",
    "        autoencoder=autoencoder,\n",
    "        data=data,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        save_path=save_path,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úÖ Training completed!\")\n",
    "        print(f\"üîß Encoder extracted successfully\")\n",
    "        print(f\"üìê Encoder output dimension: {encoding_dims[-1]}\")\n",
    "    \n",
    "    return trained_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c383b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_tensor = [torch.tensor(x, dtype=torch.float32) for x in df_binary['course_full_embeddings'].values]\n",
    "embeddings__course_tensor = torch.stack(course_tensor)\n",
    "\n",
    "user_tensor = [torch.tensor(x, dtype=torch.float32) for x in df_binary['user_full_embeddings'].values]\n",
    "embeddings_user_tensor = torch.stack(user_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_encoder = train_autoencoder_and_extract_encoder(embeddings__course_tensor, embeddings__course_tensor.shape[1], [680, 560, 360], save_path=f'{PATH_TO_CHECKPOINTS}encoder_course.pth' ,epochs=100, lr=1e-3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f816a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder = train_autoencoder_and_extract_encoder(embeddings_user_tensor, embeddings_user_tensor.shape[1], [680, 560, 360], save_path=f'{PATH_TO_CHECKPOINTS}encoder_user.pth' ,epochs=100, lr=1e-3, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b415eb1",
   "metadata": {},
   "source": [
    "# Multimodal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65457c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_dims = {\n",
    "    'course': embeddings__course_tensor.shape[1],\n",
    "    'user': embeddings_user_tensor.shape[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = MultimodalModel(modality_dims, use_bpr=True, fusion_method='by_autoencoder',shared_dim=32, layers_per_modality=[256 ,128, 64] ,autoencoders={'course': course_encoder, 'user': user_encoder}, autoencoder_output_dim=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader for BPR training using the df_bpr_df dataframe with the columns: pos_course_full_embeddings, neg_course_full_embeddings, user_full_embeddings\n",
    "class BPRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_feat = self.df.iloc[idx]['user_full_embeddings']\n",
    "        pos_course_feat = self.df.iloc[idx]['pos_course_full_embeddings']\n",
    "        neg_course_feat = self.df.iloc[idx]['neg_course_full_embeddings']\n",
    "        return {\n",
    "            'user': torch.tensor(user_feat, dtype=torch.float),\n",
    "            'course_positive': torch.tensor(pos_course_feat, dtype=torch.float),\n",
    "            'course_negative': torch.tensor(neg_course_feat, dtype=torch.float)\n",
    "        }\n",
    "bpr_dataset = BPRDataset(df_bpr_df)\n",
    "bpr_dataloader = torch.utils.data.DataLoader(bpr_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder.train_model(\n",
    "    train_loader=bpr_dataloader,\n",
    "    epochs=5,\n",
    "    lr=1e-3,\n",
    "    device='cuda',\n",
    "    save_path=f'{PATH_TO_CHECKPOINTS}multimodal_encoder_bpr_model.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_encoder = MultimodalModel(modality_dims, use_bpr=True, fusion_method='concat',shared_dim=32, layers_per_modality=[680, 560, 360 ,256 ,128, 64] ,autoencoders=None, autoencoder_output_dim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d64382",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_encoder.train_model(\n",
    "    train_loader=bpr_dataloader,\n",
    "    epochs=5,\n",
    "    lr=1e-3,\n",
    "    device='cuda',\n",
    "    save_path=f'{PATH_TO_CHECKPOINTS}multimodal_no_encoder_bpr_model.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a17f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataloaders for binary training using the df_binary dataframe with the columns: course_full_embeddings, user_full_embeddings\n",
    "class BinaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_feat = self.df.iloc[idx]['user_full_embeddings']\n",
    "        course_feat = self.df.iloc[idx]['course_full_embeddings']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        return {\n",
    "            'user': torch.tensor(user_feat, dtype=torch.float),\n",
    "            'course_positive': torch.tensor(course_feat, dtype=torch.float),\n",
    "            'targets': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split df_binary into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "binary_dataset = BinaryDataset(train_df)\n",
    "binary_dataloader = torch.utils.data.DataLoader(binary_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = BinaryDataset(val_df)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_binary = MultimodalModel(modality_dims, use_bpr=False, fusion_method='by_autoencoder',shared_dim=32, layers_per_modality=[256 ,128, 64] ,autoencoders={'course': course_encoder, 'user': user_encoder}, autoencoder_output_dim=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b11c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_binary_no_encoder = MultimodalModel(modality_dims, use_bpr=False, fusion_method='concat',shared_dim=32, layers_per_modality=[680, 560, 360 ,256 ,128, 64] ,autoencoders=None, autoencoder_output_dim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588a0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_binary.train_model(\n",
    "    train_loader=binary_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    device='cuda',\n",
    "    save_path=f'{PATH_TO_CHECKPOINTS}multimodal_encoder_binary_model.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_binary_no_encoder.train_model(\n",
    "    train_loader=binary_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    device='cuda',\n",
    "    save_path=f'{PATH_TO_CHECKPOINTS}multimodal_no_encoder_binary_model.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd47b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test_binary = pd.read_pickle(f'{PATH_TO_DATASETS}test_binary_all_vectors_128_01_transe_seqvec.pkl')\n",
    "df_test_bpr = pd.read_pickle(f'{PATH_TO_DATASETS}test_bpr_all_vectors_128_01_transe_seqvec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bpr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff67094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of user_id to user_sequence_embedding from the train dataset\n",
    "user_sequence_mapping = df_binary.drop_duplicates(subset=\"user_id\").set_index('user_id')['user_full_embeddings'].to_dict()\n",
    "\n",
    "# Replace the user_sequence_embedding in the test dataset using the mapping\n",
    "df_test_binary['user_full_embeddings'] = df_test_binary['user_id'].map(user_sequence_mapping)\n",
    "df_test_bpr['user_full_embeddings'] = df_test_bpr['user_id'].map(user_sequence_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a0fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_sequence_mapping = df_bpr_df.drop_duplicates(subset=\"pos_item_id\").set_index('pos_item_id')['pos_course_full_embeddings'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations_per_user(df, model, courses_dict, k=5, batch_size=64):\n",
    "    all_user_embs = torch.stack(df[\"user_full_embeddings\"].values)  # shape [num_users, dim]\n",
    "    recommendations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(all_user_embs), batch_size), desc=\"Generating recommendations\"):\n",
    "        batch = all_user_embs[i:i+batch_size]\n",
    "        batch_recs = model.generate_k_recommendations(courses_dict, batch, k=k)\n",
    "        recommendations.extend(batch_recs)\n",
    "\n",
    "    df[\"recommendations\"] = recommendations\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e538fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_1 = generate_recommendations_per_user(df_test_binary.drop_duplicates(subset=\"user_id\"),model_binary_no_encoder, course_sequence_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e807fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_1[\"recommendations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jcsanguino10/local_citation_model/Secuencial SR')\n",
    "from evaluation_metrics import calculate_average_mrr, calculate_average_precision_at_k, calculate_average_ndcg_at_k, calculate_average_custom_precision_at_k\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_citation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
