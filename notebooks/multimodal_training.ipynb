{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3c7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.chdir('/home/jcsanguino10/local_citation_model/recommender-fusion-recsys/loaders')\n",
    "from create_dataloader_sequential import (load_course_encoder)\n",
    "\n",
    "os.chdir('/home/jcsanguino10/local_citation_model/recommender-fusion-recsys/architectures/Sequence')\n",
    "from sec_transformer_pytorch import (create_model, load_pytorch_weights)\n",
    "\n",
    "os.chdir('/home/jcsanguino10/local_citation_model/recommender-fusion-recsys/architectures/Multimodal')\n",
    "from multimodal import Autoencoder, MultimodalModel\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caddf8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Change to just a particular GPU changing the enviroment variable\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    # Change to just use a particular GPU via torch\n",
    "    #torch.cuda.set_device(\"cuda:3\")\n",
    "    print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c9edd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths from the label encoder dicts \n",
    "\n",
    "PATH_TO_LABEL_ENCODER = '/home/jcsanguino10/local_citation_model/data/processed/'\n",
    "\n",
    "# Path to folder with datasets\n",
    "\n",
    "PATH_TO_DATASETS = '/home/jcsanguino10/local_citation_model/data/'\n",
    "\n",
    "# Path to folder with checkpoints best models\n",
    "\n",
    "PATH_TO_CHECKPOINTS = '/home/jcsanguino10/local_citation_model/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44862572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing mappings from /home/jcsanguino10/local_citation_model/data/processed\n"
     ]
    }
   ],
   "source": [
    "label_encoder, dicts = load_course_encoder('/home', PATH_TO_LABEL_ENCODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da2c5f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_encoder.classes_)  # List of all course IDs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177798f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_binary = pd.read_pickle(f'{PATH_TO_DATASETS}train_binary_all_vectors_128_01_transe_seqvec.pkl')\n",
    "df_bpr_df = pd.read_pickle(f'{PATH_TO_DATASETS}train_bpr_all_vectors_128_01_transe_seqvec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c7dec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_seq</th>\n",
       "      <th>item_text_embedding</th>\n",
       "      <th>user_text_embedding</th>\n",
       "      <th>user_cum_text_embedding</th>\n",
       "      <th>item_bpr_embedding</th>\n",
       "      <th>user_bpr_embedding</th>\n",
       "      <th>item_graph_embedding</th>\n",
       "      <th>user_graph_embedding</th>\n",
       "      <th>label</th>\n",
       "      <th>full_item_seq</th>\n",
       "      <th>parsed_item_seq</th>\n",
       "      <th>user_sequence_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6863</td>\n",
       "      <td>[6863]</td>\n",
       "      <td>[2.2087095e-05, 0.01357965, 0.013899612, -0.03...</td>\n",
       "      <td>[-0.022005824, -0.033391304, -0.013403211, -0....</td>\n",
       "      <td>[2.2087095e-05, 0.01357965, 0.013899612, -0.03...</td>\n",
       "      <td>[-0.06554511, 0.082159385, -0.024771133, -0.08...</td>\n",
       "      <td>[-0.16485311, 0.13305487, -0.109800704, -0.159...</td>\n",
       "      <td>[0.027043026, -0.028548103, -0.005808171, -0.1...</td>\n",
       "      <td>[-0.21868221, 0.107340455, 0.20054282, 0.15184...</td>\n",
       "      <td>1</td>\n",
       "      <td>[6863, 6864]</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[-3.2515903, -4.4112086, 1.4216669, -2.70002, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6864</td>\n",
       "      <td>[6863, 6864]</td>\n",
       "      <td>[-0.044033736, -0.08036226, -0.040706035, -0.0...</td>\n",
       "      <td>[-0.022005824, -0.033391304, -0.013403211, -0....</td>\n",
       "      <td>[-0.022005824, -0.033391304, -0.013403211, -0....</td>\n",
       "      <td>[-0.09407239, 0.08107624, -0.042453628, -0.100...</td>\n",
       "      <td>[-0.16485311, 0.13305487, -0.109800704, -0.159...</td>\n",
       "      <td>[-0.05497769, -0.049202707, -0.13520204, -0.09...</td>\n",
       "      <td>[-0.21868221, 0.107340455, 0.20054282, 0.15184...</td>\n",
       "      <td>1</td>\n",
       "      <td>[6863, 6864]</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[-3.2515903, -4.4112086, 1.4216669, -2.70002, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6865</td>\n",
       "      <td>[6865]</td>\n",
       "      <td>[0.06792896, -0.059695832, -0.013778767, 0.005...</td>\n",
       "      <td>[0.034128785, -0.054559205, 0.0015806267, -0.0...</td>\n",
       "      <td>[0.06792896, -0.059695832, -0.013778767, 0.005...</td>\n",
       "      <td>[-0.109637335, 0.08800741, -0.114228204, -0.05...</td>\n",
       "      <td>[-0.15681404, 0.14787957, -0.14133789, -0.1282...</td>\n",
       "      <td>[0.061120644, 0.047605906, 0.1170312, 0.092423...</td>\n",
       "      <td>[0.16515407, 0.18278724, -0.07278056, -0.06131...</td>\n",
       "      <td>1</td>\n",
       "      <td>[6865, 6866]</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>[-0.3152992, -1.0941633, 1.9535718, -1.170749,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6866</td>\n",
       "      <td>[6865, 6866]</td>\n",
       "      <td>[0.00032860466, -0.049422577, 0.01694002, -0.0...</td>\n",
       "      <td>[0.034128785, -0.054559205, 0.0015806267, -0.0...</td>\n",
       "      <td>[0.034128785, -0.054559205, 0.0015806267, -0.0...</td>\n",
       "      <td>[-0.05689768, 0.085094474, -0.04034279, -0.072...</td>\n",
       "      <td>[-0.15681404, 0.14787957, -0.14133789, -0.1282...</td>\n",
       "      <td>[-0.10074902, 0.19556874, 0.08297272, -0.14672...</td>\n",
       "      <td>[0.16515407, 0.18278724, -0.07278056, -0.06131...</td>\n",
       "      <td>1</td>\n",
       "      <td>[6865, 6866]</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>[-0.3152992, -1.0941633, 1.9535718, -1.170749,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>6867</td>\n",
       "      <td>[6867]</td>\n",
       "      <td>[-0.0041599325, -0.058022052, 0.0056412797, -0...</td>\n",
       "      <td>[0.01798853, -0.038172975, -0.0087622935, -0.0...</td>\n",
       "      <td>[-0.0041599325, -0.058022052, 0.0056412797, -0...</td>\n",
       "      <td>[0.07457628, -0.06646576, 0.07522122, 0.040138...</td>\n",
       "      <td>[-0.12063337, 0.13376932, -0.13144864, -0.0975...</td>\n",
       "      <td>[-0.07019904, -0.19594775, -0.1563805, -0.0412...</td>\n",
       "      <td>[-0.21761492, 0.1274874, 0.19495715, -0.179376...</td>\n",
       "      <td>1</td>\n",
       "      <td>[6867, 6868, 6869, 6870, 6871, 6872, 6873, 687...</td>\n",
       "      <td>[6, 7, 8, 9, 10, 11, 12, 13, 14, 3, 15, 2, 16,...</td>\n",
       "      <td>[-0.97801095, -1.112409, 0.3110269, -0.4820084...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id      item_seq  \\\n",
       "0        0     6863        [6863]   \n",
       "1        0     6864  [6863, 6864]   \n",
       "2        1     6865        [6865]   \n",
       "3        1     6866  [6865, 6866]   \n",
       "4        2     6867        [6867]   \n",
       "\n",
       "                                 item_text_embedding  \\\n",
       "0  [2.2087095e-05, 0.01357965, 0.013899612, -0.03...   \n",
       "1  [-0.044033736, -0.08036226, -0.040706035, -0.0...   \n",
       "2  [0.06792896, -0.059695832, -0.013778767, 0.005...   \n",
       "3  [0.00032860466, -0.049422577, 0.01694002, -0.0...   \n",
       "4  [-0.0041599325, -0.058022052, 0.0056412797, -0...   \n",
       "\n",
       "                                 user_text_embedding  \\\n",
       "0  [-0.022005824, -0.033391304, -0.013403211, -0....   \n",
       "1  [-0.022005824, -0.033391304, -0.013403211, -0....   \n",
       "2  [0.034128785, -0.054559205, 0.0015806267, -0.0...   \n",
       "3  [0.034128785, -0.054559205, 0.0015806267, -0.0...   \n",
       "4  [0.01798853, -0.038172975, -0.0087622935, -0.0...   \n",
       "\n",
       "                             user_cum_text_embedding  \\\n",
       "0  [2.2087095e-05, 0.01357965, 0.013899612, -0.03...   \n",
       "1  [-0.022005824, -0.033391304, -0.013403211, -0....   \n",
       "2  [0.06792896, -0.059695832, -0.013778767, 0.005...   \n",
       "3  [0.034128785, -0.054559205, 0.0015806267, -0.0...   \n",
       "4  [-0.0041599325, -0.058022052, 0.0056412797, -0...   \n",
       "\n",
       "                                  item_bpr_embedding  \\\n",
       "0  [-0.06554511, 0.082159385, -0.024771133, -0.08...   \n",
       "1  [-0.09407239, 0.08107624, -0.042453628, -0.100...   \n",
       "2  [-0.109637335, 0.08800741, -0.114228204, -0.05...   \n",
       "3  [-0.05689768, 0.085094474, -0.04034279, -0.072...   \n",
       "4  [0.07457628, -0.06646576, 0.07522122, 0.040138...   \n",
       "\n",
       "                                  user_bpr_embedding  \\\n",
       "0  [-0.16485311, 0.13305487, -0.109800704, -0.159...   \n",
       "1  [-0.16485311, 0.13305487, -0.109800704, -0.159...   \n",
       "2  [-0.15681404, 0.14787957, -0.14133789, -0.1282...   \n",
       "3  [-0.15681404, 0.14787957, -0.14133789, -0.1282...   \n",
       "4  [-0.12063337, 0.13376932, -0.13144864, -0.0975...   \n",
       "\n",
       "                                item_graph_embedding  \\\n",
       "0  [0.027043026, -0.028548103, -0.005808171, -0.1...   \n",
       "1  [-0.05497769, -0.049202707, -0.13520204, -0.09...   \n",
       "2  [0.061120644, 0.047605906, 0.1170312, 0.092423...   \n",
       "3  [-0.10074902, 0.19556874, 0.08297272, -0.14672...   \n",
       "4  [-0.07019904, -0.19594775, -0.1563805, -0.0412...   \n",
       "\n",
       "                                user_graph_embedding  label  \\\n",
       "0  [-0.21868221, 0.107340455, 0.20054282, 0.15184...      1   \n",
       "1  [-0.21868221, 0.107340455, 0.20054282, 0.15184...      1   \n",
       "2  [0.16515407, 0.18278724, -0.07278056, -0.06131...      1   \n",
       "3  [0.16515407, 0.18278724, -0.07278056, -0.06131...      1   \n",
       "4  [-0.21761492, 0.1274874, 0.19495715, -0.179376...      1   \n",
       "\n",
       "                                       full_item_seq  \\\n",
       "0                                       [6863, 6864]   \n",
       "1                                       [6863, 6864]   \n",
       "2                                       [6865, 6866]   \n",
       "3                                       [6865, 6866]   \n",
       "4  [6867, 6868, 6869, 6870, 6871, 6872, 6873, 687...   \n",
       "\n",
       "                                     parsed_item_seq  \\\n",
       "0                                             [2, 3]   \n",
       "1                                             [2, 3]   \n",
       "2                                             [4, 5]   \n",
       "3                                             [4, 5]   \n",
       "4  [6, 7, 8, 9, 10, 11, 12, 13, 14, 3, 15, 2, 16,...   \n",
       "\n",
       "                             user_sequence_embedding  \n",
       "0  [-3.2515903, -4.4112086, 1.4216669, -2.70002, ...  \n",
       "1  [-3.2515903, -4.4112086, 1.4216669, -2.70002, ...  \n",
       "2  [-0.3152992, -1.0941633, 1.9535718, -1.170749,...  \n",
       "3  [-0.3152992, -1.0941633, 1.9535718, -1.170749,...  \n",
       "4  [-0.97801095, -1.112409, 0.3110269, -0.4820084...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa3b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_columns_to_tensor(df, columns, new_column_name):\n",
    "    \"\"\"\n",
    "    Concatenates specified columns in a DataFrame and creates a tensor.\n",
    "    The resulting tensor is saved in a new column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to concatenate.\n",
    "        new_column_name (str): Name of the new column to store the tensor.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the new column containing tensors.\n",
    "    \"\"\"\n",
    "    df[new_column_name] = df[columns].apply(\n",
    "        lambda row: torch.tensor([item for col in columns for item in row[col]], dtype=torch.float),\n",
    "        axis=1\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b183f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = concat_columns_to_tensor(df_binary, ['item_text_embedding', 'item_bpr_embedding', 'item_graph_embedding'], 'course_full_embeddings')\n",
    "df_binary = concat_columns_to_tensor(df_binary, ['user_text_embedding', 'user_bpr_embedding', 'user_graph_embedding', 'user_sequence_embedding'], 'user_full_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d9d51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bpr_df = concat_columns_to_tensor(df_bpr_df, ['pos_item_text_embedding', 'pos_item_bpr_embedding', 'pos_item_graph_embedding'], 'pos_course_full_embeddings')\n",
    "df_bpr_df = concat_columns_to_tensor(df_bpr_df, ['neg_item_text_embedding', 'neg_item_bpr_embedding', 'neg_item_graph_embedding'], 'neg_course_full_embeddings')\n",
    "df_bpr_df = concat_columns_to_tensor(df_bpr_df, ['user_text_embedding', 'user_bpr_embedding', 'user_graph_embedding', 'user_sequence_embedding'], 'user_full_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092eee90",
   "metadata": {},
   "source": [
    "# Autoencoder training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dcd662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder_and_extract_encoder(data, input_dim, encoding_dims, epochs=50, lr=1e-3, \n",
    "                                         save_path=None, device='cuda', verbose=True):\n",
    "    \"\"\"\n",
    "    Train an autoencoder.\n",
    "    \n",
    "    Args:\n",
    "        data (torch.Tensor): Training data tensor of shape (batch_size, input_dim)\n",
    "        input_dim (int): Dimension of input features\n",
    "        encoding_dims (list): List of hidden layer dimensions for encoder\n",
    "                             Example: [512, 256, 128] for 3-layer encoder\n",
    "        epochs (int): Number of training epochs\n",
    "        lr (float): Learning rate\n",
    "        save_path (str): Path to save the best autoencoder model (optional)\n",
    "        device (str): Device to train on ('cpu' or 'cuda')\n",
    "        verbose (bool): Whether to print training progress\n",
    "        \n",
    "    Returns:\n",
    "        encoder (nn.Module): The trained autoencoder model\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"🚀 Starting autoencoder training...\")\n",
    "        print(f\"Input dimension: {input_dim}\")\n",
    "        print(f\"Encoding dimensions: {encoding_dims}\")\n",
    "        print(f\"Final encoding dimension: {encoding_dims[-1]}\")\n",
    "        print(f\"Training data shape: {data.shape}\")\n",
    "    \n",
    "    # Create autoencoder\n",
    "    autoencoder = Autoencoder(input_dim=input_dim, encoding_dims=encoding_dims)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"📊 Autoencoder architecture created\")\n",
    "        print(f\"Encoder layers: {len(autoencoder.encoder)}\")\n",
    "        print(f\"Decoder layers: {len(autoencoder.decoder)}\")\n",
    "    \n",
    "    # Train the autoencoder using the enhanced train_autoencoder method\n",
    "    trained_autoencoder = Autoencoder.train_autoencoder(\n",
    "        autoencoder=autoencoder,\n",
    "        data=data,\n",
    "        epochs=epochs,\n",
    "        lr=lr,\n",
    "        save_path=save_path,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"✅ Training completed!\")\n",
    "        print(f\"🔧 Encoder extracted successfully\")\n",
    "        print(f\"📐 Encoder output dimension: {encoding_dims[-1]}\")\n",
    "    \n",
    "    return trained_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c383b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2986274/1407629466.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  course_tensor = [torch.tensor(x, dtype=torch.float32) for x in df_binary['course_full_embeddings'].values]\n",
      "/tmp/ipykernel_2986274/1407629466.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_tensor = [torch.tensor(x, dtype=torch.float32) for x in df_binary['user_full_embeddings'].values]\n"
     ]
    }
   ],
   "source": [
    "course_tensor = [torch.tensor(x, dtype=torch.float32) for x in df_binary['course_full_embeddings'].values]\n",
    "embeddings__course_tensor = torch.stack(course_tensor)\n",
    "\n",
    "user_tensor = [torch.tensor(x, dtype=torch.float32) for x in df_binary['user_full_embeddings'].values]\n",
    "embeddings_user_tensor = torch.stack(user_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9abd9a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: 31456 training samples, 7864 validation samples\n",
      "Training autoencoder for 100 epochs...\n",
      "✅ New best model saved at epoch 1 with val_loss: 0.002894\n",
      "Epoch 1/100, Train Loss: 0.003350, Val Loss: 0.002894, Best Val Loss: 0.002894\n",
      "✅ New best model saved at epoch 2 with val_loss: 0.002669\n",
      "✅ New best model saved at epoch 3 with val_loss: 0.002537\n",
      "✅ New best model saved at epoch 4 with val_loss: 0.002485\n",
      "✅ New best model saved at epoch 5 with val_loss: 0.002439\n",
      "Epoch 5/100, Train Loss: 0.002535, Val Loss: 0.002439, Best Val Loss: 0.002439\n",
      "✅ New best model saved at epoch 6 with val_loss: 0.002404\n",
      "✅ New best model saved at epoch 7 with val_loss: 0.002377\n",
      "✅ New best model saved at epoch 8 with val_loss: 0.002340\n",
      "✅ New best model saved at epoch 9 with val_loss: 0.002289\n",
      "✅ New best model saved at epoch 10 with val_loss: 0.002240\n",
      "Epoch 10/100, Train Loss: 0.002324, Val Loss: 0.002240, Best Val Loss: 0.002240\n",
      "✅ New best model saved at epoch 11 with val_loss: 0.002192\n",
      "✅ New best model saved at epoch 12 with val_loss: 0.002149\n",
      "✅ New best model saved at epoch 13 with val_loss: 0.002099\n",
      "✅ New best model saved at epoch 14 with val_loss: 0.002067\n",
      "✅ New best model saved at epoch 15 with val_loss: 0.002042\n",
      "Epoch 15/100, Train Loss: 0.002120, Val Loss: 0.002042, Best Val Loss: 0.002042\n",
      "✅ New best model saved at epoch 16 with val_loss: 0.002032\n",
      "✅ New best model saved at epoch 17 with val_loss: 0.002013\n",
      "✅ New best model saved at epoch 18 with val_loss: 0.001995\n",
      "✅ New best model saved at epoch 19 with val_loss: 0.001979\n",
      "✅ New best model saved at epoch 20 with val_loss: 0.001966\n",
      "Epoch 20/100, Train Loss: 0.002015, Val Loss: 0.001966, Best Val Loss: 0.001966\n",
      "✅ New best model saved at epoch 21 with val_loss: 0.001951\n",
      "✅ New best model saved at epoch 22 with val_loss: 0.001931\n",
      "✅ New best model saved at epoch 23 with val_loss: 0.001915\n",
      "✅ New best model saved at epoch 24 with val_loss: 0.001897\n",
      "✅ New best model saved at epoch 25 with val_loss: 0.001878\n",
      "Epoch 25/100, Train Loss: 0.001930, Val Loss: 0.001878, Best Val Loss: 0.001878\n",
      "✅ New best model saved at epoch 26 with val_loss: 0.001861\n",
      "✅ New best model saved at epoch 27 with val_loss: 0.001849\n",
      "✅ New best model saved at epoch 28 with val_loss: 0.001842\n",
      "✅ New best model saved at epoch 29 with val_loss: 0.001825\n",
      "✅ New best model saved at epoch 30 with val_loss: 0.001814\n",
      "Epoch 30/100, Train Loss: 0.001860, Val Loss: 0.001814, Best Val Loss: 0.001814\n",
      "✅ New best model saved at epoch 31 with val_loss: 0.001806\n",
      "✅ New best model saved at epoch 32 with val_loss: 0.001796\n",
      "✅ New best model saved at epoch 33 with val_loss: 0.001781\n",
      "✅ New best model saved at epoch 34 with val_loss: 0.001770\n",
      "✅ New best model saved at epoch 35 with val_loss: 0.001761\n",
      "Epoch 35/100, Train Loss: 0.001805, Val Loss: 0.001761, Best Val Loss: 0.001761\n",
      "✅ New best model saved at epoch 36 with val_loss: 0.001748\n",
      "✅ New best model saved at epoch 37 with val_loss: 0.001735\n",
      "✅ New best model saved at epoch 38 with val_loss: 0.001725\n",
      "✅ New best model saved at epoch 39 with val_loss: 0.001717\n",
      "✅ New best model saved at epoch 40 with val_loss: 0.001704\n",
      "Epoch 40/100, Train Loss: 0.001751, Val Loss: 0.001704, Best Val Loss: 0.001704\n",
      "✅ New best model saved at epoch 41 with val_loss: 0.001692\n",
      "✅ New best model saved at epoch 42 with val_loss: 0.001683\n",
      "✅ New best model saved at epoch 43 with val_loss: 0.001671\n",
      "✅ New best model saved at epoch 44 with val_loss: 0.001658\n",
      "✅ New best model saved at epoch 45 with val_loss: 0.001649\n",
      "Epoch 45/100, Train Loss: 0.001697, Val Loss: 0.001649, Best Val Loss: 0.001649\n",
      "✅ New best model saved at epoch 46 with val_loss: 0.001638\n",
      "✅ New best model saved at epoch 47 with val_loss: 0.001624\n",
      "✅ New best model saved at epoch 48 with val_loss: 0.001613\n",
      "✅ New best model saved at epoch 49 with val_loss: 0.001602\n",
      "✅ New best model saved at epoch 50 with val_loss: 0.001588\n",
      "Epoch 50/100, Train Loss: 0.001641, Val Loss: 0.001588, Best Val Loss: 0.001588\n",
      "✅ New best model saved at epoch 51 with val_loss: 0.001577\n",
      "✅ New best model saved at epoch 52 with val_loss: 0.001563\n",
      "✅ New best model saved at epoch 53 with val_loss: 0.001550\n",
      "✅ New best model saved at epoch 54 with val_loss: 0.001538\n",
      "✅ New best model saved at epoch 55 with val_loss: 0.001528\n",
      "Epoch 55/100, Train Loss: 0.001581, Val Loss: 0.001528, Best Val Loss: 0.001528\n",
      "✅ New best model saved at epoch 56 with val_loss: 0.001515\n",
      "✅ New best model saved at epoch 57 with val_loss: 0.001511\n",
      "✅ New best model saved at epoch 58 with val_loss: 0.001487\n",
      "✅ New best model saved at epoch 59 with val_loss: 0.001474\n",
      "✅ New best model saved at epoch 60 with val_loss: 0.001470\n",
      "Epoch 60/100, Train Loss: 0.001523, Val Loss: 0.001470, Best Val Loss: 0.001470\n",
      "✅ New best model saved at epoch 61 with val_loss: 0.001450\n",
      "✅ New best model saved at epoch 62 with val_loss: 0.001437\n",
      "✅ New best model saved at epoch 63 with val_loss: 0.001429\n",
      "✅ New best model saved at epoch 64 with val_loss: 0.001412\n",
      "✅ New best model saved at epoch 65 with val_loss: 0.001399\n",
      "Epoch 65/100, Train Loss: 0.001463, Val Loss: 0.001399, Best Val Loss: 0.001399\n",
      "✅ New best model saved at epoch 66 with val_loss: 0.001390\n",
      "✅ New best model saved at epoch 67 with val_loss: 0.001373\n",
      "✅ New best model saved at epoch 68 with val_loss: 0.001360\n",
      "✅ New best model saved at epoch 69 with val_loss: 0.001351\n",
      "✅ New best model saved at epoch 70 with val_loss: 0.001335\n",
      "Epoch 70/100, Train Loss: 0.001405, Val Loss: 0.001335, Best Val Loss: 0.001335\n",
      "✅ New best model saved at epoch 71 with val_loss: 0.001320\n",
      "✅ New best model saved at epoch 72 with val_loss: 0.001311\n",
      "✅ New best model saved at epoch 73 with val_loss: 0.001296\n",
      "✅ New best model saved at epoch 74 with val_loss: 0.001282\n",
      "✅ New best model saved at epoch 75 with val_loss: 0.001273\n",
      "Epoch 75/100, Train Loss: 0.001345, Val Loss: 0.001273, Best Val Loss: 0.001273\n",
      "✅ New best model saved at epoch 76 with val_loss: 0.001258\n",
      "✅ New best model saved at epoch 77 with val_loss: 0.001244\n",
      "✅ New best model saved at epoch 78 with val_loss: 0.001234\n",
      "✅ New best model saved at epoch 79 with val_loss: 0.001222\n",
      "✅ New best model saved at epoch 80 with val_loss: 0.001207\n",
      "Epoch 80/100, Train Loss: 0.001289, Val Loss: 0.001207, Best Val Loss: 0.001207\n",
      "✅ New best model saved at epoch 81 with val_loss: 0.001196\n",
      "✅ New best model saved at epoch 82 with val_loss: 0.001184\n",
      "✅ New best model saved at epoch 83 with val_loss: 0.001171\n",
      "✅ New best model saved at epoch 84 with val_loss: 0.001158\n",
      "✅ New best model saved at epoch 85 with val_loss: 0.001146\n",
      "Epoch 85/100, Train Loss: 0.001232, Val Loss: 0.001146, Best Val Loss: 0.001146\n",
      "✅ New best model saved at epoch 86 with val_loss: 0.001133\n",
      "✅ New best model saved at epoch 87 with val_loss: 0.001121\n",
      "✅ New best model saved at epoch 88 with val_loss: 0.001109\n",
      "✅ New best model saved at epoch 89 with val_loss: 0.001096\n",
      "✅ New best model saved at epoch 90 with val_loss: 0.001084\n",
      "Epoch 90/100, Train Loss: 0.001178, Val Loss: 0.001084, Best Val Loss: 0.001084\n",
      "✅ New best model saved at epoch 91 with val_loss: 0.001073\n",
      "✅ New best model saved at epoch 92 with val_loss: 0.001058\n",
      "✅ New best model saved at epoch 93 with val_loss: 0.001046\n",
      "✅ New best model saved at epoch 94 with val_loss: 0.001035\n",
      "✅ New best model saved at epoch 95 with val_loss: 0.001021\n",
      "Epoch 95/100, Train Loss: 0.001122, Val Loss: 0.001021, Best Val Loss: 0.001021\n",
      "✅ New best model saved at epoch 96 with val_loss: 0.001008\n",
      "✅ New best model saved at epoch 97 with val_loss: 0.000996\n",
      "✅ New best model saved at epoch 98 with val_loss: 0.000983\n",
      "✅ New best model saved at epoch 99 with val_loss: 0.000973\n",
      "✅ New best model saved at epoch 100 with val_loss: 0.000958\n",
      "Epoch 100/100, Train Loss: 0.001067, Val Loss: 0.000958, Best Val Loss: 0.000958\n",
      "🎯 Training completed. Best validation loss: 0.000958\n"
     ]
    }
   ],
   "source": [
    "course_encoder = train_autoencoder_and_extract_encoder(embeddings__course_tensor, embeddings__course_tensor.shape[1], [680, 560, 360], save_path=f'{PATH_TO_CHECKPOINTS}encoder_course.pth' ,epochs=100, lr=1e-3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f816a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: 31456 training samples, 7864 validation samples\n",
      "Training autoencoder for 100 epochs...\n",
      "✅ New best model saved at epoch 1 with val_loss: 0.266960\n",
      "Epoch 1/100, Train Loss: 0.273164, Val Loss: 0.266960, Best Val Loss: 0.266960\n",
      "✅ New best model saved at epoch 2 with val_loss: 0.249679\n",
      "✅ New best model saved at epoch 3 with val_loss: 0.206341\n",
      "✅ New best model saved at epoch 5 with val_loss: 0.152368\n",
      "Epoch 5/100, Train Loss: 0.267226, Val Loss: 0.152368, Best Val Loss: 0.152368\n",
      "Epoch 10/100, Train Loss: 0.184694, Val Loss: 0.167825, Best Val Loss: 0.152368\n",
      "✅ New best model saved at epoch 11 with val_loss: 0.150768\n",
      "✅ New best model saved at epoch 12 with val_loss: 0.144200\n",
      "✅ New best model saved at epoch 13 with val_loss: 0.141376\n",
      "✅ New best model saved at epoch 14 with val_loss: 0.127704\n",
      "✅ New best model saved at epoch 15 with val_loss: 0.121834\n",
      "Epoch 15/100, Train Loss: 0.138565, Val Loss: 0.121834, Best Val Loss: 0.121834\n",
      "✅ New best model saved at epoch 19 with val_loss: 0.118558\n",
      "✅ New best model saved at epoch 20 with val_loss: 0.117254\n",
      "Epoch 20/100, Train Loss: 0.123981, Val Loss: 0.117254, Best Val Loss: 0.117254\n",
      "✅ New best model saved at epoch 22 with val_loss: 0.115975\n",
      "✅ New best model saved at epoch 23 with val_loss: 0.114518\n",
      "Epoch 25/100, Train Loss: 0.119879, Val Loss: 0.116148, Best Val Loss: 0.114518\n",
      "✅ New best model saved at epoch 27 with val_loss: 0.112179\n",
      "✅ New best model saved at epoch 28 with val_loss: 0.110675\n",
      "✅ New best model saved at epoch 29 with val_loss: 0.110286\n",
      "✅ New best model saved at epoch 30 with val_loss: 0.109266\n",
      "Epoch 30/100, Train Loss: 0.116574, Val Loss: 0.109266, Best Val Loss: 0.109266\n",
      "✅ New best model saved at epoch 31 with val_loss: 0.108307\n",
      "✅ New best model saved at epoch 33 with val_loss: 0.107784\n",
      "✅ New best model saved at epoch 34 with val_loss: 0.105477\n",
      "✅ New best model saved at epoch 35 with val_loss: 0.102698\n",
      "Epoch 35/100, Train Loss: 0.109809, Val Loss: 0.102698, Best Val Loss: 0.102698\n",
      "✅ New best model saved at epoch 36 with val_loss: 0.100453\n",
      "✅ New best model saved at epoch 37 with val_loss: 0.097977\n",
      "✅ New best model saved at epoch 38 with val_loss: 0.095484\n",
      "✅ New best model saved at epoch 39 with val_loss: 0.094019\n",
      "✅ New best model saved at epoch 40 with val_loss: 0.091836\n",
      "Epoch 40/100, Train Loss: 0.099084, Val Loss: 0.091836, Best Val Loss: 0.091836\n",
      "✅ New best model saved at epoch 41 with val_loss: 0.089160\n",
      "✅ New best model saved at epoch 42 with val_loss: 0.087589\n",
      "✅ New best model saved at epoch 43 with val_loss: 0.086909\n",
      "✅ New best model saved at epoch 44 with val_loss: 0.086893\n",
      "✅ New best model saved at epoch 45 with val_loss: 0.085875\n",
      "Epoch 45/100, Train Loss: 0.092513, Val Loss: 0.085875, Best Val Loss: 0.085875\n",
      "✅ New best model saved at epoch 46 with val_loss: 0.084333\n",
      "✅ New best model saved at epoch 47 with val_loss: 0.083115\n",
      "✅ New best model saved at epoch 48 with val_loss: 0.082481\n",
      "✅ New best model saved at epoch 49 with val_loss: 0.082082\n",
      "✅ New best model saved at epoch 50 with val_loss: 0.081596\n",
      "Epoch 50/100, Train Loss: 0.087792, Val Loss: 0.081596, Best Val Loss: 0.081596\n",
      "✅ New best model saved at epoch 51 with val_loss: 0.081169\n",
      "✅ New best model saved at epoch 52 with val_loss: 0.080808\n",
      "✅ New best model saved at epoch 53 with val_loss: 0.080509\n",
      "✅ New best model saved at epoch 54 with val_loss: 0.080207\n",
      "✅ New best model saved at epoch 55 with val_loss: 0.079713\n",
      "Epoch 55/100, Train Loss: 0.085621, Val Loss: 0.079713, Best Val Loss: 0.079713\n",
      "✅ New best model saved at epoch 56 with val_loss: 0.079289\n",
      "✅ New best model saved at epoch 57 with val_loss: 0.079096\n",
      "✅ New best model saved at epoch 58 with val_loss: 0.079032\n",
      "✅ New best model saved at epoch 59 with val_loss: 0.078892\n",
      "✅ New best model saved at epoch 60 with val_loss: 0.078516\n",
      "Epoch 60/100, Train Loss: 0.084113, Val Loss: 0.078516, Best Val Loss: 0.078516\n",
      "✅ New best model saved at epoch 61 with val_loss: 0.078233\n",
      "✅ New best model saved at epoch 62 with val_loss: 0.078120\n",
      "✅ New best model saved at epoch 63 with val_loss: 0.078109\n",
      "✅ New best model saved at epoch 64 with val_loss: 0.078025\n",
      "✅ New best model saved at epoch 65 with val_loss: 0.077786\n",
      "Epoch 65/100, Train Loss: 0.083008, Val Loss: 0.077786, Best Val Loss: 0.077786\n",
      "✅ New best model saved at epoch 66 with val_loss: 0.077588\n",
      "✅ New best model saved at epoch 67 with val_loss: 0.077532\n",
      "✅ New best model saved at epoch 68 with val_loss: 0.077527\n",
      "✅ New best model saved at epoch 69 with val_loss: 0.077377\n",
      "✅ New best model saved at epoch 70 with val_loss: 0.077207\n",
      "Epoch 70/100, Train Loss: 0.082269, Val Loss: 0.077207, Best Val Loss: 0.077207\n",
      "✅ New best model saved at epoch 71 with val_loss: 0.077122\n",
      "✅ New best model saved at epoch 72 with val_loss: 0.077068\n",
      "✅ New best model saved at epoch 73 with val_loss: 0.076942\n",
      "✅ New best model saved at epoch 74 with val_loss: 0.076792\n",
      "✅ New best model saved at epoch 75 with val_loss: 0.076724\n",
      "Epoch 75/100, Train Loss: 0.081572, Val Loss: 0.076724, Best Val Loss: 0.076724\n",
      "✅ New best model saved at epoch 76 with val_loss: 0.076675\n",
      "✅ New best model saved at epoch 77 with val_loss: 0.076539\n",
      "✅ New best model saved at epoch 78 with val_loss: 0.076408\n",
      "✅ New best model saved at epoch 79 with val_loss: 0.076304\n",
      "✅ New best model saved at epoch 80 with val_loss: 0.076196\n",
      "Epoch 80/100, Train Loss: 0.080825, Val Loss: 0.076196, Best Val Loss: 0.076196\n",
      "✅ New best model saved at epoch 81 with val_loss: 0.076017\n",
      "✅ New best model saved at epoch 82 with val_loss: 0.075832\n",
      "✅ New best model saved at epoch 83 with val_loss: 0.075599\n",
      "✅ New best model saved at epoch 84 with val_loss: 0.075305\n",
      "✅ New best model saved at epoch 85 with val_loss: 0.074914\n",
      "Epoch 85/100, Train Loss: 0.079736, Val Loss: 0.074914, Best Val Loss: 0.074914\n",
      "✅ New best model saved at epoch 86 with val_loss: 0.074508\n",
      "✅ New best model saved at epoch 87 with val_loss: 0.073979\n",
      "✅ New best model saved at epoch 88 with val_loss: 0.073325\n",
      "✅ New best model saved at epoch 89 with val_loss: 0.072519\n",
      "✅ New best model saved at epoch 90 with val_loss: 0.071637\n",
      "Epoch 90/100, Train Loss: 0.077148, Val Loss: 0.071637, Best Val Loss: 0.071637\n",
      "✅ New best model saved at epoch 91 with val_loss: 0.070720\n",
      "✅ New best model saved at epoch 92 with val_loss: 0.069910\n",
      "✅ New best model saved at epoch 93 with val_loss: 0.069245\n",
      "✅ New best model saved at epoch 94 with val_loss: 0.068614\n",
      "✅ New best model saved at epoch 95 with val_loss: 0.068309\n",
      "Epoch 95/100, Train Loss: 0.073703, Val Loss: 0.068309, Best Val Loss: 0.068309\n",
      "✅ New best model saved at epoch 96 with val_loss: 0.067703\n",
      "✅ New best model saved at epoch 97 with val_loss: 0.067498\n",
      "✅ New best model saved at epoch 98 with val_loss: 0.066942\n",
      "✅ New best model saved at epoch 99 with val_loss: 0.066532\n",
      "✅ New best model saved at epoch 100 with val_loss: 0.066305\n",
      "Epoch 100/100, Train Loss: 0.071518, Val Loss: 0.066305, Best Val Loss: 0.066305\n",
      "🎯 Training completed. Best validation loss: 0.066305\n"
     ]
    }
   ],
   "source": [
    "user_encoder = train_autoencoder_and_extract_encoder(embeddings_user_tensor, embeddings_user_tensor.shape[1], [680, 560, 360], save_path=f'{PATH_TO_CHECKPOINTS}encoder_user.pth' ,epochs=100, lr=1e-3, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b415eb1",
   "metadata": {},
   "source": [
    "# Multimodal training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c65457c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_dims = {\n",
    "    'course': embeddings__course_tensor.shape[1],\n",
    "    'user': embeddings_user_tensor.shape[1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5519a170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User feature layer input dim: 360\n",
      "Course feature layer input dim: 360\n"
     ]
    }
   ],
   "source": [
    "model_encoder = MultimodalModel(modality_dims, use_bpr=True, fusion_method='by_autoencoder',shared_dim=32, layers_per_modality=[256 ,128, 64] ,autoencoders={'course': course_encoder, 'user': user_encoder}, autoencoder_output_dim=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af23d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader for BPR training using the df_bpr_df dataframe with the columns: pos_course_full_embeddings, neg_course_full_embeddings, user_full_embeddings\n",
    "class BPRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_feat = self.df.iloc[idx]['user_full_embeddings']\n",
    "        pos_course_feat = self.df.iloc[idx]['pos_course_full_embeddings']\n",
    "        neg_course_feat = self.df.iloc[idx]['neg_course_full_embeddings']\n",
    "        return {\n",
    "            'user': torch.tensor(user_feat, dtype=torch.float),\n",
    "            'course_positive': torch.tensor(pos_course_feat, dtype=torch.float),\n",
    "            'course_negative': torch.tensor(neg_course_feat, dtype=torch.float)\n",
    "        }\n",
    "bpr_dataset = BPRDataset(df_bpr_df)\n",
    "bpr_dataloader = torch.utils.data.DataLoader(bpr_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79d1e1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multimodal model for 20 epochs...\n",
      "No validation dataset provided - using training loss for model selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2986274/242066864.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'user': torch.tensor(user_feat, dtype=torch.float),\n",
      "/tmp/ipykernel_2986274/242066864.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'course_positive': torch.tensor(pos_course_feat, dtype=torch.float),\n",
      "/tmp/ipykernel_2986274/242066864.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'course_negative': torch.tensor(neg_course_feat, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved at epoch 1 with val_loss: 0.511510\n",
      "Epoch 1/20, Train Loss: 0.511510, Best Loss: 0.511510\n",
      "✅ New best model saved at epoch 2 with val_loss: 0.406950\n",
      "Epoch 2/20, Train Loss: 0.406950, Best Loss: 0.406950\n",
      "✅ New best model saved at epoch 3 with val_loss: 0.358461\n",
      "Epoch 3/20, Train Loss: 0.358461, Best Loss: 0.358461\n",
      "✅ New best model saved at epoch 4 with val_loss: 0.336161\n",
      "Epoch 4/20, Train Loss: 0.336161, Best Loss: 0.336161\n",
      "✅ New best model saved at epoch 5 with val_loss: 0.322812\n",
      "Epoch 5/20, Train Loss: 0.322812, Best Loss: 0.322812\n",
      "✅ New best model saved at epoch 6 with val_loss: 0.318244\n",
      "Epoch 6/20, Train Loss: 0.318244, Best Loss: 0.318244\n",
      "✅ New best model saved at epoch 7 with val_loss: 0.312302\n",
      "Epoch 7/20, Train Loss: 0.312302, Best Loss: 0.312302\n",
      "✅ New best model saved at epoch 8 with val_loss: 0.305751\n",
      "Epoch 8/20, Train Loss: 0.305751, Best Loss: 0.305751\n",
      "✅ New best model saved at epoch 9 with val_loss: 0.300331\n",
      "Epoch 9/20, Train Loss: 0.300331, Best Loss: 0.300331\n",
      "✅ New best model saved at epoch 10 with val_loss: 0.294878\n",
      "Epoch 10/20, Train Loss: 0.294878, Best Loss: 0.294878\n",
      "✅ New best model saved at epoch 11 with val_loss: 0.291470\n",
      "Epoch 11/20, Train Loss: 0.291470, Best Loss: 0.291470\n",
      "✅ New best model saved at epoch 12 with val_loss: 0.287946\n",
      "Epoch 12/20, Train Loss: 0.287946, Best Loss: 0.287946\n",
      "✅ New best model saved at epoch 13 with val_loss: 0.287192\n",
      "Epoch 13/20, Train Loss: 0.287192, Best Loss: 0.287192\n",
      "✅ New best model saved at epoch 14 with val_loss: 0.284686\n",
      "Epoch 14/20, Train Loss: 0.284686, Best Loss: 0.284686\n",
      "✅ New best model saved at epoch 15 with val_loss: 0.280893\n",
      "Epoch 15/20, Train Loss: 0.280893, Best Loss: 0.280893\n",
      "✅ New best model saved at epoch 16 with val_loss: 0.279466\n",
      "Epoch 16/20, Train Loss: 0.279466, Best Loss: 0.279466\n",
      "✅ New best model saved at epoch 17 with val_loss: 0.276812\n",
      "Epoch 17/20, Train Loss: 0.276812, Best Loss: 0.276812\n",
      "✅ New best model saved at epoch 18 with val_loss: 0.275190\n",
      "Epoch 18/20, Train Loss: 0.275190, Best Loss: 0.275190\n",
      "✅ New best model saved at epoch 19 with val_loss: 0.272749\n",
      "Epoch 19/20, Train Loss: 0.272749, Best Loss: 0.272749\n",
      "✅ New best model saved at epoch 20 with val_loss: 0.270854\n",
      "Epoch 20/20, Train Loss: 0.270854, Best Loss: 0.270854\n",
      "🎯 Training completed. Best training loss: 0.270854\n"
     ]
    }
   ],
   "source": [
    "model_encoder.train_model(\n",
    "    train_loader=bpr_dataloader,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    device='cuda',\n",
    "    save_path=f'{PATH_TO_CHECKPOINTS}multimodal_encoder_bpr_model.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91c5277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User feature layer input dim: 1158\n",
      "Course feature layer input dim: 960\n"
     ]
    }
   ],
   "source": [
    "model_no_encoder = MultimodalModel(modality_dims, use_bpr=True, fusion_method='concat',shared_dim=32, layers_per_modality=[680, 560, 360 ,256 ,128, 64] ,autoencoders=None, autoencoder_output_dim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d64382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multimodal model for 20 epochs...\n",
      "No validation dataset provided - using training loss for model selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2986274/242066864.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'user': torch.tensor(user_feat, dtype=torch.float),\n",
      "/tmp/ipykernel_2986274/242066864.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'course_positive': torch.tensor(pos_course_feat, dtype=torch.float),\n",
      "/tmp/ipykernel_2986274/242066864.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'course_negative': torch.tensor(neg_course_feat, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved at epoch 1 with val_loss: 0.522595\n",
      "Epoch 1/20, Train Loss: 0.522595, Best Loss: 0.522595\n",
      "✅ New best model saved at epoch 2 with val_loss: 0.435717\n",
      "Epoch 2/20, Train Loss: 0.435717, Best Loss: 0.435717\n",
      "✅ New best model saved at epoch 3 with val_loss: 0.414116\n",
      "Epoch 3/20, Train Loss: 0.414116, Best Loss: 0.414116\n",
      "✅ New best model saved at epoch 4 with val_loss: 0.345178\n",
      "Epoch 4/20, Train Loss: 0.345178, Best Loss: 0.345178\n",
      "✅ New best model saved at epoch 5 with val_loss: 0.287274\n",
      "Epoch 5/20, Train Loss: 0.287274, Best Loss: 0.287274\n",
      "✅ New best model saved at epoch 6 with val_loss: 0.252212\n",
      "Epoch 6/20, Train Loss: 0.252212, Best Loss: 0.252212\n",
      "✅ New best model saved at epoch 7 with val_loss: 0.215344\n",
      "Epoch 7/20, Train Loss: 0.215344, Best Loss: 0.215344\n",
      "✅ New best model saved at epoch 8 with val_loss: 0.176293\n",
      "Epoch 8/20, Train Loss: 0.176293, Best Loss: 0.176293\n",
      "✅ New best model saved at epoch 9 with val_loss: 0.162344\n",
      "Epoch 9/20, Train Loss: 0.162344, Best Loss: 0.162344\n",
      "✅ New best model saved at epoch 10 with val_loss: 0.158067\n",
      "Epoch 10/20, Train Loss: 0.158067, Best Loss: 0.158067\n",
      "✅ New best model saved at epoch 11 with val_loss: 0.149515\n",
      "Epoch 11/20, Train Loss: 0.149515, Best Loss: 0.149515\n",
      "✅ New best model saved at epoch 12 with val_loss: 0.148373\n",
      "Epoch 12/20, Train Loss: 0.148373, Best Loss: 0.148373\n",
      "✅ New best model saved at epoch 13 with val_loss: 0.147382\n",
      "Epoch 13/20, Train Loss: 0.147382, Best Loss: 0.147382\n",
      "✅ New best model saved at epoch 14 with val_loss: 0.142709\n",
      "Epoch 14/20, Train Loss: 0.142709, Best Loss: 0.142709\n",
      "✅ New best model saved at epoch 15 with val_loss: 0.140107\n",
      "Epoch 15/20, Train Loss: 0.140107, Best Loss: 0.140107\n",
      "✅ New best model saved at epoch 16 with val_loss: 0.138298\n",
      "Epoch 16/20, Train Loss: 0.138298, Best Loss: 0.138298\n",
      "✅ New best model saved at epoch 17 with val_loss: 0.138062\n",
      "Epoch 17/20, Train Loss: 0.138062, Best Loss: 0.138062\n",
      "✅ New best model saved at epoch 18 with val_loss: 0.136539\n",
      "Epoch 18/20, Train Loss: 0.136539, Best Loss: 0.136539\n",
      "✅ New best model saved at epoch 19 with val_loss: 0.136267\n",
      "Epoch 19/20, Train Loss: 0.136267, Best Loss: 0.136267\n",
      "✅ New best model saved at epoch 20 with val_loss: 0.134573\n",
      "Epoch 20/20, Train Loss: 0.134573, Best Loss: 0.134573\n",
      "🎯 Training completed. Best training loss: 0.134573\n"
     ]
    }
   ],
   "source": [
    "model_no_encoder.train_model(\n",
    "    train_loader=bpr_dataloader,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    device='cuda',\n",
    "    save_path=f'{PATH_TO_CHECKPOINTS}multimodal_no_encoder_bpr_model.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92a17f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataloaders for binary training using the df_binary dataframe with the columns: course_full_embeddings, user_full_embeddings\n",
    "class BinaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_feat = self.df.iloc[idx]['user_full_embeddings']\n",
    "        course_feat = self.df.iloc[idx]['course_full_embeddings']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        return {\n",
    "            'user': torch.tensor(user_feat, dtype=torch.float),\n",
    "            'course_positive': torch.tensor(course_feat, dtype=torch.float),\n",
    "            'targets': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ce7ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split df_binary into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "binary_dataset = BinaryDataset(train_df)\n",
    "binary_dataloader = torch.utils.data.DataLoader(binary_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = BinaryDataset(val_df)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dbe02ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User feature layer input dim: 360\n",
      "Course feature layer input dim: 360\n"
     ]
    }
   ],
   "source": [
    "model_binary = MultimodalModel(modality_dims, use_bpr=False, fusion_method='by_autoencoder',shared_dim=32, layers_per_modality=[256 ,128, 64] ,autoencoders={'course': course_encoder, 'user': user_encoder}, autoencoder_output_dim=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d3b11c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User feature layer input dim: 1158\n",
      "Course feature layer input dim: 960\n"
     ]
    }
   ],
   "source": [
    "model_binary_no_encoder = MultimodalModel(modality_dims, use_bpr=False, fusion_method='concat',shared_dim=32, layers_per_modality=[680, 560, 360 ,256 ,128, 64] ,autoencoders=None, autoencoder_output_dim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "588a0a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multimodal model for 20 epochs...\n",
      "Using validation dataset with 123 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2986274/754840888.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'user': torch.tensor(user_feat, dtype=torch.float),\n",
      "/tmp/ipykernel_2986274/754840888.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'course_positive': torch.tensor(course_feat, dtype=torch.float),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved at epoch 1 with val_loss: 0.461161\n",
      "Epoch 1/20, Train Loss: 0.481609, Val Loss: 0.461161, Best Val Loss: 0.461161\n",
      "Epoch 2/20, Train Loss: 0.456511, Val Loss: 0.463141, Best Val Loss: 0.461161\n",
      "Epoch 3/20, Train Loss: 0.450777, Val Loss: 0.468032, Best Val Loss: 0.461161\n",
      "✅ New best model saved at epoch 4 with val_loss: 0.455969\n",
      "Epoch 4/20, Train Loss: 0.449362, Val Loss: 0.455969, Best Val Loss: 0.455969\n",
      "✅ New best model saved at epoch 5 with val_loss: 0.451952\n",
      "Epoch 5/20, Train Loss: 0.448433, Val Loss: 0.451952, Best Val Loss: 0.451952\n",
      "✅ New best model saved at epoch 6 with val_loss: 0.448255\n",
      "Epoch 6/20, Train Loss: 0.445316, Val Loss: 0.448255, Best Val Loss: 0.448255\n",
      "✅ New best model saved at epoch 7 with val_loss: 0.444425\n",
      "Epoch 7/20, Train Loss: 0.442667, Val Loss: 0.444425, Best Val Loss: 0.444425\n",
      "Epoch 8/20, Train Loss: 0.440303, Val Loss: 0.456076, Best Val Loss: 0.444425\n",
      "✅ New best model saved at epoch 9 with val_loss: 0.443938\n",
      "Epoch 9/20, Train Loss: 0.436844, Val Loss: 0.443938, Best Val Loss: 0.443938\n",
      "✅ New best model saved at epoch 10 with val_loss: 0.442536\n",
      "Epoch 10/20, Train Loss: 0.436485, Val Loss: 0.442536, Best Val Loss: 0.442536\n",
      "✅ New best model saved at epoch 11 with val_loss: 0.438092\n",
      "Epoch 11/20, Train Loss: 0.434371, Val Loss: 0.438092, Best Val Loss: 0.438092\n",
      "Epoch 12/20, Train Loss: 0.431864, Val Loss: 0.442948, Best Val Loss: 0.438092\n",
      "✅ New best model saved at epoch 13 with val_loss: 0.437636\n",
      "Epoch 13/20, Train Loss: 0.429688, Val Loss: 0.437636, Best Val Loss: 0.437636\n",
      "✅ New best model saved at epoch 14 with val_loss: 0.435848\n",
      "Epoch 14/20, Train Loss: 0.428420, Val Loss: 0.435848, Best Val Loss: 0.435848\n",
      "Epoch 15/20, Train Loss: 0.427819, Val Loss: 0.440073, Best Val Loss: 0.435848\n",
      "✅ New best model saved at epoch 16 with val_loss: 0.435595\n",
      "Epoch 16/20, Train Loss: 0.426821, Val Loss: 0.435595, Best Val Loss: 0.435595\n",
      "✅ New best model saved at epoch 17 with val_loss: 0.433906\n",
      "Epoch 17/20, Train Loss: 0.425265, Val Loss: 0.433906, Best Val Loss: 0.433906\n",
      "✅ New best model saved at epoch 18 with val_loss: 0.432311\n",
      "Epoch 18/20, Train Loss: 0.423693, Val Loss: 0.432311, Best Val Loss: 0.432311\n",
      "✅ New best model saved at epoch 19 with val_loss: 0.428067\n",
      "Epoch 19/20, Train Loss: 0.423224, Val Loss: 0.428067, Best Val Loss: 0.428067\n",
      "✅ New best model saved at epoch 20 with val_loss: 0.426965\n",
      "Epoch 20/20, Train Loss: 0.421417, Val Loss: 0.426965, Best Val Loss: 0.426965\n",
      "🎯 Training completed. Best validation loss: 0.426965\n"
     ]
    }
   ],
   "source": [
    "model_binary.train_model(\n",
    "    train_loader=binary_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    device='cuda',\n",
    "    save_path=f'{PATH_TO_CHECKPOINTS}multimodal_encoder_binary_model.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93e0ce64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multimodal model for 20 epochs...\n",
      "Using validation dataset with 123 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2986274/754840888.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'user': torch.tensor(user_feat, dtype=torch.float),\n",
      "/tmp/ipykernel_2986274/754840888.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'course_positive': torch.tensor(course_feat, dtype=torch.float),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New best model saved at epoch 1 with val_loss: 0.522323\n",
      "Epoch 1/20, Train Loss: 0.536095, Val Loss: 0.522323, Best Val Loss: 0.522323\n",
      "✅ New best model saved at epoch 2 with val_loss: 0.406445\n",
      "Epoch 2/20, Train Loss: 0.453521, Val Loss: 0.406445, Best Val Loss: 0.406445\n",
      "✅ New best model saved at epoch 3 with val_loss: 0.375662\n",
      "Epoch 3/20, Train Loss: 0.394084, Val Loss: 0.375662, Best Val Loss: 0.375662\n",
      "✅ New best model saved at epoch 4 with val_loss: 0.374218\n",
      "Epoch 4/20, Train Loss: 0.379946, Val Loss: 0.374218, Best Val Loss: 0.374218\n",
      "Epoch 5/20, Train Loss: 0.372543, Val Loss: 0.393146, Best Val Loss: 0.374218\n",
      "✅ New best model saved at epoch 6 with val_loss: 0.369182\n",
      "Epoch 6/20, Train Loss: 0.365980, Val Loss: 0.369182, Best Val Loss: 0.369182\n",
      "✅ New best model saved at epoch 7 with val_loss: 0.367399\n",
      "Epoch 7/20, Train Loss: 0.362869, Val Loss: 0.367399, Best Val Loss: 0.367399\n",
      "✅ New best model saved at epoch 8 with val_loss: 0.355549\n",
      "Epoch 8/20, Train Loss: 0.358092, Val Loss: 0.355549, Best Val Loss: 0.355549\n",
      "Epoch 9/20, Train Loss: 0.352354, Val Loss: 0.364642, Best Val Loss: 0.355549\n",
      "Epoch 10/20, Train Loss: 0.353649, Val Loss: 0.363066, Best Val Loss: 0.355549\n",
      "Epoch 11/20, Train Loss: 0.351248, Val Loss: 0.377297, Best Val Loss: 0.355549\n",
      "Epoch 12/20, Train Loss: 0.347866, Val Loss: 0.358623, Best Val Loss: 0.355549\n",
      "Epoch 13/20, Train Loss: 0.347054, Val Loss: 0.358097, Best Val Loss: 0.355549\n",
      "Epoch 14/20, Train Loss: 0.344888, Val Loss: 0.365992, Best Val Loss: 0.355549\n",
      "Epoch 15/20, Train Loss: 0.344542, Val Loss: 0.357551, Best Val Loss: 0.355549\n",
      "Epoch 16/20, Train Loss: 0.340564, Val Loss: 0.373857, Best Val Loss: 0.355549\n",
      "✅ New best model saved at epoch 17 with val_loss: 0.351462\n",
      "Epoch 17/20, Train Loss: 0.344007, Val Loss: 0.351462, Best Val Loss: 0.351462\n",
      "Epoch 18/20, Train Loss: 0.338657, Val Loss: 0.353466, Best Val Loss: 0.351462\n",
      "Epoch 19/20, Train Loss: 0.338118, Val Loss: 0.359963, Best Val Loss: 0.351462\n",
      "Epoch 20/20, Train Loss: 0.336579, Val Loss: 0.353900, Best Val Loss: 0.351462\n",
      "🎯 Training completed. Best validation loss: 0.351462\n"
     ]
    }
   ],
   "source": [
    "model_binary_no_encoder.train_model(\n",
    "    train_loader=binary_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    device='cuda',\n",
    "    save_path=f'{PATH_TO_CHECKPOINTS}multimodal_no_encoder_binary_model.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dd47b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test_binary = pd.read_pickle(f'{PATH_TO_DATASETS}test_binary_all_vectors_128_01_transe_seqvec.pkl')\n",
    "df_test_bpr = pd.read_pickle(f'{PATH_TO_DATASETS}test_bpr_all_vectors_128_01_transe_seqvec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7fcecfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>pos_item_id</th>\n",
       "      <th>neg_item_id</th>\n",
       "      <th>item_seq</th>\n",
       "      <th>pos_item_text_embedding</th>\n",
       "      <th>neg_item_text_embedding</th>\n",
       "      <th>user_text_embedding</th>\n",
       "      <th>user_cum_text_embedding</th>\n",
       "      <th>pos_item_bpr_embedding</th>\n",
       "      <th>neg_item_bpr_embedding</th>\n",
       "      <th>user_bpr_embedding</th>\n",
       "      <th>pos_item_graph_embedding</th>\n",
       "      <th>neg_item_graph_embedding</th>\n",
       "      <th>user_graph_embedding</th>\n",
       "      <th>full_item_seq</th>\n",
       "      <th>parsed_item_seq</th>\n",
       "      <th>user_sequence_embedding</th>\n",
       "      <th>pos_course_full_embeddings</th>\n",
       "      <th>neg_course_full_embeddings</th>\n",
       "      <th>user_full_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6863</td>\n",
       "      <td>7029</td>\n",
       "      <td>[6863]</td>\n",
       "      <td>[2.2087095e-05, 0.01357965, 0.013899612, -0.03...</td>\n",
       "      <td>[0.005838169, 0.0019245448, 0.013981204, 0.049...</td>\n",
       "      <td>[-0.022005824, -0.033391304, -0.013403211, -0....</td>\n",
       "      <td>[2.2087095e-05, 0.01357965, 0.013899612, -0.03...</td>\n",
       "      <td>[-0.06554511, 0.082159385, -0.024771133, -0.08...</td>\n",
       "      <td>[0.12648226, -0.08695751, 0.1032631, 0.1059599...</td>\n",
       "      <td>[-0.16485311, 0.13305487, -0.109800704, -0.159...</td>\n",
       "      <td>[0.027043026, -0.028548103, -0.005808171, -0.1...</td>\n",
       "      <td>[-0.14888977, 0.050079968, 0.12822092, 0.00475...</td>\n",
       "      <td>[-0.21868221, 0.107340455, 0.20054282, 0.15184...</td>\n",
       "      <td>[6863, 6864]</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[-3.2515903, -4.4112086, 1.4216669, -2.70002, ...</td>\n",
       "      <td>[tensor(2.2087e-05), tensor(0.0136), tensor(0....</td>\n",
       "      <td>[tensor(0.0058), tensor(0.0019), tensor(0.0140...</td>\n",
       "      <td>[tensor(-0.0220), tensor(-0.0334), tensor(-0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6864</td>\n",
       "      <td>7029</td>\n",
       "      <td>[6863, 6864]</td>\n",
       "      <td>[-0.044033736, -0.08036226, -0.040706035, -0.0...</td>\n",
       "      <td>[0.005838169, 0.0019245448, 0.013981204, 0.049...</td>\n",
       "      <td>[-0.022005824, -0.033391304, -0.013403211, -0....</td>\n",
       "      <td>[-0.022005824, -0.033391304, -0.013403211, -0....</td>\n",
       "      <td>[-0.09407239, 0.08107624, -0.042453628, -0.100...</td>\n",
       "      <td>[0.12648226, -0.08695751, 0.1032631, 0.1059599...</td>\n",
       "      <td>[-0.16485311, 0.13305487, -0.109800704, -0.159...</td>\n",
       "      <td>[-0.05497769, -0.049202707, -0.13520204, -0.09...</td>\n",
       "      <td>[-0.14888977, 0.050079968, 0.12822092, 0.00475...</td>\n",
       "      <td>[-0.21868221, 0.107340455, 0.20054282, 0.15184...</td>\n",
       "      <td>[6863, 6864]</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[-3.2515903, -4.4112086, 1.4216669, -2.70002, ...</td>\n",
       "      <td>[tensor(-0.0440), tensor(-0.0804), tensor(-0.0...</td>\n",
       "      <td>[tensor(0.0058), tensor(0.0019), tensor(0.0140...</td>\n",
       "      <td>[tensor(-0.0220), tensor(-0.0334), tensor(-0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6865</td>\n",
       "      <td>6923</td>\n",
       "      <td>[6865]</td>\n",
       "      <td>[0.06792896, -0.059695832, -0.013778767, 0.005...</td>\n",
       "      <td>[0.050885703, 0.016394274, -0.0133330505, -0.0...</td>\n",
       "      <td>[0.034128785, -0.054559205, 0.0015806267, -0.0...</td>\n",
       "      <td>[0.06792896, -0.059695832, -0.013778767, 0.005...</td>\n",
       "      <td>[-0.109637335, 0.08800741, -0.114228204, -0.05...</td>\n",
       "      <td>[0.10082436, -0.09402634, 0.100806765, 0.09215...</td>\n",
       "      <td>[-0.15681404, 0.14787957, -0.14133789, -0.1282...</td>\n",
       "      <td>[0.061120644, 0.047605906, 0.1170312, 0.092423...</td>\n",
       "      <td>[0.12331399, 0.03431532, 0.21699251, 0.1532834...</td>\n",
       "      <td>[0.16515407, 0.18278724, -0.07278056, -0.06131...</td>\n",
       "      <td>[6865, 6866]</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>[-0.3152992, -1.0941633, 1.9535718, -1.170749,...</td>\n",
       "      <td>[tensor(0.0679), tensor(-0.0597), tensor(-0.01...</td>\n",
       "      <td>[tensor(0.0509), tensor(0.0164), tensor(-0.013...</td>\n",
       "      <td>[tensor(0.0341), tensor(-0.0546), tensor(0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6866</td>\n",
       "      <td>6928</td>\n",
       "      <td>[6865, 6866]</td>\n",
       "      <td>[0.00032860466, -0.049422577, 0.01694002, -0.0...</td>\n",
       "      <td>[0.05351212, -0.040352862, -0.01637018, 0.0096...</td>\n",
       "      <td>[0.034128785, -0.054559205, 0.0015806267, -0.0...</td>\n",
       "      <td>[0.034128785, -0.054559205, 0.0015806267, -0.0...</td>\n",
       "      <td>[-0.05689768, 0.085094474, -0.04034279, -0.072...</td>\n",
       "      <td>[0.1240448, -0.09917505, 0.098442316, 0.094072...</td>\n",
       "      <td>[-0.15681404, 0.14787957, -0.14133789, -0.1282...</td>\n",
       "      <td>[-0.10074902, 0.19556874, 0.08297272, -0.14672...</td>\n",
       "      <td>[-0.0033504472, 0.13650173, -0.19897188, 0.152...</td>\n",
       "      <td>[0.16515407, 0.18278724, -0.07278056, -0.06131...</td>\n",
       "      <td>[6865, 6866]</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>[-0.3152992, -1.0941633, 1.9535718, -1.170749,...</td>\n",
       "      <td>[tensor(0.0003), tensor(-0.0494), tensor(0.016...</td>\n",
       "      <td>[tensor(0.0535), tensor(-0.0404), tensor(-0.01...</td>\n",
       "      <td>[tensor(0.0341), tensor(-0.0546), tensor(0.001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>6867</td>\n",
       "      <td>7047</td>\n",
       "      <td>[6867]</td>\n",
       "      <td>[-0.0041599325, -0.058022052, 0.0056412797, -0...</td>\n",
       "      <td>[0.036724288, -0.022930954, 0.012853282, -0.01...</td>\n",
       "      <td>[0.01798853, -0.038172975, -0.0087622935, -0.0...</td>\n",
       "      <td>[-0.0041599325, -0.058022052, 0.0056412797, -0...</td>\n",
       "      <td>[0.07457628, -0.06646576, 0.07522122, 0.040138...</td>\n",
       "      <td>[0.13138588, -0.10216947, 0.10394299, 0.092404...</td>\n",
       "      <td>[-0.12063337, 0.13376932, -0.13144864, -0.0975...</td>\n",
       "      <td>[-0.07019904, -0.19594775, -0.1563805, -0.0412...</td>\n",
       "      <td>[0.17877811, 0.13246465, 0.01112991, 0.1806746...</td>\n",
       "      <td>[-0.21761492, 0.1274874, 0.19495715, -0.179376...</td>\n",
       "      <td>[6867, 6868, 6869, 6870, 6871, 6872, 6873, 687...</td>\n",
       "      <td>[6, 7, 8, 9, 10, 11, 12, 13, 14, 3, 15, 2, 16,...</td>\n",
       "      <td>[-0.97801095, -1.112409, 0.3110269, -0.4820084...</td>\n",
       "      <td>[tensor(-0.0042), tensor(-0.0580), tensor(0.00...</td>\n",
       "      <td>[tensor(0.0367), tensor(-0.0229), tensor(0.012...</td>\n",
       "      <td>[tensor(0.0180), tensor(-0.0382), tensor(-0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  pos_item_id  neg_item_id      item_seq  \\\n",
       "0        0         6863         7029        [6863]   \n",
       "1        0         6864         7029  [6863, 6864]   \n",
       "2        1         6865         6923        [6865]   \n",
       "3        1         6866         6928  [6865, 6866]   \n",
       "4        2         6867         7047        [6867]   \n",
       "\n",
       "                             pos_item_text_embedding  \\\n",
       "0  [2.2087095e-05, 0.01357965, 0.013899612, -0.03...   \n",
       "1  [-0.044033736, -0.08036226, -0.040706035, -0.0...   \n",
       "2  [0.06792896, -0.059695832, -0.013778767, 0.005...   \n",
       "3  [0.00032860466, -0.049422577, 0.01694002, -0.0...   \n",
       "4  [-0.0041599325, -0.058022052, 0.0056412797, -0...   \n",
       "\n",
       "                             neg_item_text_embedding  \\\n",
       "0  [0.005838169, 0.0019245448, 0.013981204, 0.049...   \n",
       "1  [0.005838169, 0.0019245448, 0.013981204, 0.049...   \n",
       "2  [0.050885703, 0.016394274, -0.0133330505, -0.0...   \n",
       "3  [0.05351212, -0.040352862, -0.01637018, 0.0096...   \n",
       "4  [0.036724288, -0.022930954, 0.012853282, -0.01...   \n",
       "\n",
       "                                 user_text_embedding  \\\n",
       "0  [-0.022005824, -0.033391304, -0.013403211, -0....   \n",
       "1  [-0.022005824, -0.033391304, -0.013403211, -0....   \n",
       "2  [0.034128785, -0.054559205, 0.0015806267, -0.0...   \n",
       "3  [0.034128785, -0.054559205, 0.0015806267, -0.0...   \n",
       "4  [0.01798853, -0.038172975, -0.0087622935, -0.0...   \n",
       "\n",
       "                             user_cum_text_embedding  \\\n",
       "0  [2.2087095e-05, 0.01357965, 0.013899612, -0.03...   \n",
       "1  [-0.022005824, -0.033391304, -0.013403211, -0....   \n",
       "2  [0.06792896, -0.059695832, -0.013778767, 0.005...   \n",
       "3  [0.034128785, -0.054559205, 0.0015806267, -0.0...   \n",
       "4  [-0.0041599325, -0.058022052, 0.0056412797, -0...   \n",
       "\n",
       "                              pos_item_bpr_embedding  \\\n",
       "0  [-0.06554511, 0.082159385, -0.024771133, -0.08...   \n",
       "1  [-0.09407239, 0.08107624, -0.042453628, -0.100...   \n",
       "2  [-0.109637335, 0.08800741, -0.114228204, -0.05...   \n",
       "3  [-0.05689768, 0.085094474, -0.04034279, -0.072...   \n",
       "4  [0.07457628, -0.06646576, 0.07522122, 0.040138...   \n",
       "\n",
       "                              neg_item_bpr_embedding  \\\n",
       "0  [0.12648226, -0.08695751, 0.1032631, 0.1059599...   \n",
       "1  [0.12648226, -0.08695751, 0.1032631, 0.1059599...   \n",
       "2  [0.10082436, -0.09402634, 0.100806765, 0.09215...   \n",
       "3  [0.1240448, -0.09917505, 0.098442316, 0.094072...   \n",
       "4  [0.13138588, -0.10216947, 0.10394299, 0.092404...   \n",
       "\n",
       "                                  user_bpr_embedding  \\\n",
       "0  [-0.16485311, 0.13305487, -0.109800704, -0.159...   \n",
       "1  [-0.16485311, 0.13305487, -0.109800704, -0.159...   \n",
       "2  [-0.15681404, 0.14787957, -0.14133789, -0.1282...   \n",
       "3  [-0.15681404, 0.14787957, -0.14133789, -0.1282...   \n",
       "4  [-0.12063337, 0.13376932, -0.13144864, -0.0975...   \n",
       "\n",
       "                            pos_item_graph_embedding  \\\n",
       "0  [0.027043026, -0.028548103, -0.005808171, -0.1...   \n",
       "1  [-0.05497769, -0.049202707, -0.13520204, -0.09...   \n",
       "2  [0.061120644, 0.047605906, 0.1170312, 0.092423...   \n",
       "3  [-0.10074902, 0.19556874, 0.08297272, -0.14672...   \n",
       "4  [-0.07019904, -0.19594775, -0.1563805, -0.0412...   \n",
       "\n",
       "                            neg_item_graph_embedding  \\\n",
       "0  [-0.14888977, 0.050079968, 0.12822092, 0.00475...   \n",
       "1  [-0.14888977, 0.050079968, 0.12822092, 0.00475...   \n",
       "2  [0.12331399, 0.03431532, 0.21699251, 0.1532834...   \n",
       "3  [-0.0033504472, 0.13650173, -0.19897188, 0.152...   \n",
       "4  [0.17877811, 0.13246465, 0.01112991, 0.1806746...   \n",
       "\n",
       "                                user_graph_embedding  \\\n",
       "0  [-0.21868221, 0.107340455, 0.20054282, 0.15184...   \n",
       "1  [-0.21868221, 0.107340455, 0.20054282, 0.15184...   \n",
       "2  [0.16515407, 0.18278724, -0.07278056, -0.06131...   \n",
       "3  [0.16515407, 0.18278724, -0.07278056, -0.06131...   \n",
       "4  [-0.21761492, 0.1274874, 0.19495715, -0.179376...   \n",
       "\n",
       "                                       full_item_seq  \\\n",
       "0                                       [6863, 6864]   \n",
       "1                                       [6863, 6864]   \n",
       "2                                       [6865, 6866]   \n",
       "3                                       [6865, 6866]   \n",
       "4  [6867, 6868, 6869, 6870, 6871, 6872, 6873, 687...   \n",
       "\n",
       "                                     parsed_item_seq  \\\n",
       "0                                             [2, 3]   \n",
       "1                                             [2, 3]   \n",
       "2                                             [4, 5]   \n",
       "3                                             [4, 5]   \n",
       "4  [6, 7, 8, 9, 10, 11, 12, 13, 14, 3, 15, 2, 16,...   \n",
       "\n",
       "                             user_sequence_embedding  \\\n",
       "0  [-3.2515903, -4.4112086, 1.4216669, -2.70002, ...   \n",
       "1  [-3.2515903, -4.4112086, 1.4216669, -2.70002, ...   \n",
       "2  [-0.3152992, -1.0941633, 1.9535718, -1.170749,...   \n",
       "3  [-0.3152992, -1.0941633, 1.9535718, -1.170749,...   \n",
       "4  [-0.97801095, -1.112409, 0.3110269, -0.4820084...   \n",
       "\n",
       "                          pos_course_full_embeddings  \\\n",
       "0  [tensor(2.2087e-05), tensor(0.0136), tensor(0....   \n",
       "1  [tensor(-0.0440), tensor(-0.0804), tensor(-0.0...   \n",
       "2  [tensor(0.0679), tensor(-0.0597), tensor(-0.01...   \n",
       "3  [tensor(0.0003), tensor(-0.0494), tensor(0.016...   \n",
       "4  [tensor(-0.0042), tensor(-0.0580), tensor(0.00...   \n",
       "\n",
       "                          neg_course_full_embeddings  \\\n",
       "0  [tensor(0.0058), tensor(0.0019), tensor(0.0140...   \n",
       "1  [tensor(0.0058), tensor(0.0019), tensor(0.0140...   \n",
       "2  [tensor(0.0509), tensor(0.0164), tensor(-0.013...   \n",
       "3  [tensor(0.0535), tensor(-0.0404), tensor(-0.01...   \n",
       "4  [tensor(0.0367), tensor(-0.0229), tensor(0.012...   \n",
       "\n",
       "                                user_full_embeddings  \n",
       "0  [tensor(-0.0220), tensor(-0.0334), tensor(-0.0...  \n",
       "1  [tensor(-0.0220), tensor(-0.0334), tensor(-0.0...  \n",
       "2  [tensor(0.0341), tensor(-0.0546), tensor(0.001...  \n",
       "3  [tensor(0.0341), tensor(-0.0546), tensor(0.001...  \n",
       "4  [tensor(0.0180), tensor(-0.0382), tensor(-0.00...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bpr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ff67094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of user_id to user_sequence_embedding from the train dataset\n",
    "user_sequence_mapping = df_binary.drop_duplicates(subset=\"user_id\").set_index('user_id')['user_full_embeddings'].to_dict()\n",
    "\n",
    "# Replace the user_sequence_embedding in the test dataset using the mapping\n",
    "df_test_binary['user_full_embeddings'] = df_test_binary['user_id'].map(user_sequence_mapping)\n",
    "df_test_bpr['user_full_embeddings'] = df_test_bpr['user_id'].map(user_sequence_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35a0fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_sequence_mapping = df_bpr_df.drop_duplicates(subset=\"pos_item_id\").set_index('pos_item_id')['pos_course_full_embeddings'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8072d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/jcsanguino10/local_citation_model/Secuencial SR')\n",
    "from evaluation_metrics import calculate_average_mrr, calculate_average_precision_at_k, calculate_average_ndcg_at_k, calculate_average_custom_precision_at_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9205b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations_per_user(df, model, courses_dict, k=5, batch_size=64):\n",
    "\n",
    "    user_tensors = [torch.tensor(x) if not isinstance(x, torch.Tensor) else x \n",
    "                for x in df[\"user_full_embeddings\"].values]\n",
    "\n",
    "    user_tensors = torch.stack(user_tensors)\n",
    "\n",
    "    all_user_embs = user_tensors  # shape [num_users, dim]\n",
    "    recommendations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(all_user_embs), batch_size), desc=\"Generating recommendations\"):\n",
    "        batch = all_user_embs[i:i+batch_size]\n",
    "        batch_recs = model.generate_k_recommendations(courses_dict, batch, k=k)\n",
    "        recommendations.extend(batch_recs)\n",
    "\n",
    "    df[\"recommendations\"] = recommendations\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b63a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(df, models, courses_dict, k):\n",
    "    for model in models:\n",
    "        temp_df = generate_recommendations_per_user(df.drop_duplicates(subset=\"user_id\"), model, courses_dict)\n",
    "        k=5\n",
    "        courses_test_dataset = temp_df[\"full_item_seq\"].to_list()\n",
    "        courses_recommended_list = temp_df[\"recommendations\"].to_list()\n",
    "\n",
    "        avg_mrr = calculate_average_mrr(courses_test_dataset, courses_recommended_list)\n",
    "        avg_ndcg_at_k = calculate_average_ndcg_at_k(courses_test_dataset, courses_recommended_list, k)\n",
    "        avg_precision_at_k = calculate_average_precision_at_k(courses_test_dataset, courses_recommended_list, k)\n",
    "        avg_custom_precision_at_k = calculate_average_custom_precision_at_k(courses_test_dataset, courses_recommended_list, k)\n",
    "        print(f\"Average MRR: {avg_mrr}\")\n",
    "        print(f\"Average NDCG@{k}: {avg_ndcg_at_k}\")\n",
    "        print(f\"Average Precision@{k}: {avg_precision_at_k}\")\n",
    "        print(f\"Average Custom Precision@{k}: {avg_custom_precision_at_k}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f619832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations:   0%|          | 0/108 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 108/108 [00:06<00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MRR: 0.12431152557190916\n",
      "Average NDCG@5: 0.17511222410590072\n",
      "Average Precision@5: 0.06825003642721604\n",
      "Average Custom Precision@5: 0.20988634707853718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 108/108 [00:06<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MRR: 0.1178007674000692\n",
      "Average NDCG@5: 0.15994722831188687\n",
      "Average Precision@5: 0.059565787556460324\n",
      "Average Custom Precision@5: 0.18412987517606497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 108/108 [00:07<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MRR: 0.13320462382825968\n",
      "Average NDCG@5: 0.18586588563020812\n",
      "Average Precision@5: 0.07221331779105097\n",
      "Average Custom Precision@5: 0.22111564427606975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating recommendations: 100%|██████████| 108/108 [00:07<00:00, 14.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MRR: 0.13147797367526512\n",
      "Average NDCG@5: 0.1746645209448283\n",
      "Average Precision@5: 0.06192627130992068\n",
      "Average Custom Precision@5: 0.19377580261304597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(df_test_binary.drop_duplicates(subset=\"user_id\"), models=[model_encoder, model_no_encoder, model_binary, model_binary_no_encoder], courses_dict=course_sequence_mapping, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_citation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
